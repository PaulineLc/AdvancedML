{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Calculate the precision score for both of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_precision(true_positive, false_positive):\n",
    "    return true_positive / (true_positive + false_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision score for class A is 0.79\n",
      "The precision score for class B is 0.63\n"
     ]
    }
   ],
   "source": [
    "#A\n",
    "q1_precision_a = calculate_precision(407, 108)\n",
    "print(\"The precision score for class A is\", round(q1_precision_a, 2))\n",
    "\n",
    "#B\n",
    "q1_precision_b = calculate_precision(160, 93)\n",
    "print(\"The precision score for class B is\", round(q1_precision_b, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Calculate the recall score for both of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_recall(true_positive, false_negative):\n",
    "    return true_positive / (true_positive + false_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recall score for class A is 0.81\n",
      "The recall score for class B is 0.6\n"
     ]
    }
   ],
   "source": [
    "#A\n",
    "q1_recall_a = calculate_recall(407, 93)\n",
    "print(\"The recall score for class A is\", round(q1_recall_a, 2))\n",
    "\n",
    "#B\n",
    "q1_recall_b = calculate_recall(160, 108)\n",
    "print(\"The recall score for class B is\", round(q1_recall_b, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Calculate the F1-measure score for both of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_f1_measure(true_positive, false_positive, false_negative):\n",
    "    precision = calculate_precision(true_positive, false_positive)\n",
    "    recall = calculate_recall(true_positive, false_negative)\n",
    "    return (2 * precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1-measure score for class A is 0.8\n",
      "The F1-measure score for class B is 0.6\n"
     ]
    }
   ],
   "source": [
    "#A\n",
    "q1_f1_a = calculate_f1_measure(407, 108, 93)\n",
    "print(\"The F1-measure score for class A is\", round(q1_f1_a, 2))\n",
    "\n",
    "#B\n",
    "q1_f1_b = calculate_f1_measure(160, 93, 108)\n",
    "print(\"The F1-measure score for class B is\", round(q1_recall_b, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Calculate the overall classification accuracy for all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_class_accuracy(true_positive, true_negative, false_positive, false_negative):\n",
    "    return (true_positive+true_negative) / (true_positive + true_negative + false_positive + false_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall classification accuracy is 0.74\n"
     ]
    }
   ],
   "source": [
    "q1_accuracy = calculate_class_accuracy(407, 160, 108, 93)\n",
    "print(\"The overall classification accuracy is\", round(q1_accuracy, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Example TrueClassLabel KNN_Prediction J48_Prediction SVM_Prediction\n",
      "0         1           spam           spam           spam           spam\n",
      "1         2       non-spam       non-spam           spam       non-spam\n",
      "2         3           spam       non-spam       non-spam           spam\n",
      "3         4       non-spam       non-spam       non-spam       non-spam\n",
      "4         5           spam           spam           spam           spam\n",
      "5         6       non-spam       non-spam       non-spam       non-spam\n",
      "6         7       non-spam           spam           spam       non-spam\n",
      "7         8       non-spam       non-spam           spam           spam\n",
      "8         9           spam           spam       non-spam           spam\n",
      "9        10           spam           spam       non-spam       non-spam\n",
      "10       11           spam       non-spam       non-spam           spam\n",
      "11       12           spam           spam           spam           spam\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Tutorials/tuto4_table1.txt', sep=\" \")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Calculate the overall accuracy for each of the classifiers on this data. Based on your calculations, which classifier the most accurate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN classification accuracy:\t 0.75\n",
      "J48 classification accuracy:\t 0.42\n",
      "SVM classification accuracy:\t 0.83\n"
     ]
    }
   ],
   "source": [
    "q2_accuracy_knn = calculate_class_accuracy(true_positive=df[(df['KNN_Prediction'] == 'spam') & (df['TrueClassLabel'] == 'spam')].shape[0],\n",
    "                                           true_negative=df[(df['KNN_Prediction'] == 'non-spam') & (df['TrueClassLabel'] == 'non-spam')].shape[0],\n",
    "                                           false_positive=df[(df['KNN_Prediction'] == 'spam') & (df['TrueClassLabel'] == 'non-spam')].shape[0],\n",
    "                                           false_negative=df[(df['KNN_Prediction'] == 'non-spam') & (df['TrueClassLabel'] == 'spam')].shape[0])\n",
    "\n",
    "print(\"KNN classification accuracy:\\t\", round(q2_accuracy_knn, 2))\n",
    "\n",
    "\n",
    "q2_accuracy_j48 = calculate_class_accuracy(true_positive=df[(df['J48_Prediction'] == 'spam') & (df['TrueClassLabel'] == 'spam')].shape[0],\n",
    "                                           true_negative=df[(df['J48_Prediction'] == 'non-spam') & (df['TrueClassLabel'] == 'non-spam')].shape[0],\n",
    "                                           false_positive=df[(df['J48_Prediction'] == 'spam') & (df['TrueClassLabel'] == 'non-spam')].shape[0],\n",
    "                                           false_negative=df[(df['J48_Prediction'] == 'non-spam') & (df['TrueClassLabel'] == 'spam')].shape[0])\n",
    "\n",
    "print(\"J48 classification accuracy:\\t\", round(q2_accuracy_j48, 2))\n",
    "\n",
    "\n",
    "q2_accuracy_svm = calculate_class_accuracy(true_positive=df[(df['SVM_Prediction'] == 'spam') & (df['TrueClassLabel'] == 'spam')].shape[0],\n",
    "                                           true_negative=df[(df['SVM_Prediction'] == 'non-spam') & (df['TrueClassLabel'] == 'non-spam')].shape[0],\n",
    "                                           false_positive=df[(df['SVM_Prediction'] == 'spam') & (df['TrueClassLabel'] == 'non-spam')].shape[0],\n",
    "                                           false_negative=df[(df['SVM_Prediction'] == 'non-spam') & (df['TrueClassLabel'] == 'spam')].shape[0])\n",
    "\n",
    "print(\"SVM classification accuracy:\\t\", round(q2_accuracy_svm, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Calculate the precision of each classifier relative to the “spam” class. Based on your calculations, which classifier achieves the highest precision for this class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN precision score:\t 0.83\n",
      "J48 precision score:\t 0.5\n",
      "SVM precision score:\t 0.86\n"
     ]
    }
   ],
   "source": [
    "q2_precision_knn = calculate_precision(true_positive= df[(df['KNN_Prediction'] == 'spam') & (df['TrueClassLabel'] == 'spam')].shape[0], \n",
    "                                       false_positive= df[(df['KNN_Prediction'] == 'spam') & (df['TrueClassLabel'] == 'non-spam')].shape[0])\n",
    "\n",
    "print(\"KNN precision score:\\t\", round(q2_precision_knn, 2))\n",
    "\n",
    "\n",
    "q2_precision_j48 = calculate_precision(true_positive= df[(df['J48_Prediction'] == 'spam') & (df['TrueClassLabel'] == 'spam')].shape[0], \n",
    "                                       false_positive= df[(df['J48_Prediction'] == 'spam') & (df['TrueClassLabel'] == 'non-spam')].shape[0])\n",
    "\n",
    "print(\"J48 precision score:\\t\", round(q2_precision_j48, 2))\n",
    "\n",
    "\n",
    "q2_precision_svm = calculate_precision(true_positive= df[(df['SVM_Prediction'] == 'spam') & (df['TrueClassLabel'] == 'spam')].shape[0], \n",
    "                                       false_positive= df[(df['SVM_Prediction'] == 'spam') & (df['TrueClassLabel'] == 'non-spam')].shape[0])\n",
    "\n",
    "print(\"SVM precision score:\\t\", round(q2_precision_svm, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "The table below shows the number of correct and incorrect predictions made by an image classifier during a 10-fold cross validation experiment, where the goal was to classify 500 images into one of three categories: {cats, dogs, people}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Cat_Correct  Cat_Incorrect  Dog_Correct  Dog_Incorrect  People_Correct  \\\n",
      "1            82             68           82             68             164   \n",
      "2            81             69          102             48             176   \n",
      "3            99             51           97             53             160   \n",
      "4            81             69          102             48             148   \n",
      "5            94             56           99             51             148   \n",
      "6            97             53           91             59             162   \n",
      "7            81             69           94             56             148   \n",
      "8            76             74           79             71             181   \n",
      "9            76             74           97             53             160   \n",
      "10           96             54           79             71             179   \n",
      "\n",
      "    People_Incorrect  \n",
      "1                 36  \n",
      "2                 24  \n",
      "3                 40  \n",
      "4                 52  \n",
      "5                 52  \n",
      "6                 38  \n",
      "7                 52  \n",
      "8                 19  \n",
      "9                 40  \n",
      "10                21  \n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv('Tutorials/tuto4_table2.txt', sep=\" \")\n",
    "\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) What is the overall accuracy of the classifier based on the cross-validation results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall accuracy of the classifier is 0.68\n"
     ]
    }
   ],
   "source": [
    "q3_accuracy = df2[['Cat_Correct', 'Dog_Correct', 'People_Correct']].sum().sum() / df2.sum().sum()\n",
    "print(\"The overall accuracy of the classifier is\", round(q3_accuracy, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) What conclusion might be draw about the different classes in the data, based on the results above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the class accuracy of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the class \"Cat\" (<3 Cats! <3) is\t 0.58\n",
      "The accuracy for the class \"Dog\" is\t\t\t 0.61\n",
      "The accuracy for the class \"People\" is\t\t\t 0.81\n"
     ]
    }
   ],
   "source": [
    "q3_accuracy_cats = sum(df2['Cat_Correct']) / df2[['Cat_Correct', 'Cat_Incorrect']].sum().sum()\n",
    "print(\"The accuracy for the class \\\"Cat\\\" (<3 Cats! <3) is\\t\", round(q3_accuracy_cats, 2))\n",
    "\n",
    "q3_accuracy_dogs = sum(df2['Dog_Correct']) / df2[['Dog_Correct', 'Dog_Incorrect']].sum().sum()\n",
    "print(\"The accuracy for the class \\\"Dog\\\" is\\t\\t\\t\", round(q3_accuracy_dogs, 2))\n",
    "\n",
    "q3_accuracy_people = sum(df2['People_Correct']) / df2[['People_Correct', 'People_Incorrect']].sum().sum()\n",
    "print(\"The accuracy for the class \\\"People\\\" is\\t\\t\\t\", round(q3_accuracy_people, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High accuracy for class \"People\", low accuracy for \"Cats\" and \"Dogs\". Suggests system is poor at distinguishing between these classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Would “leave-one-out cross validation” be an appropriate evaluation strategy on this dataset? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If each fold has 500 examples, then overall dataset has n=5000 examples.\n",
    "So leave-one-out would require running 5000 experiments where 1 example is left out each time.\n",
    "Could be computationally impractical to do this, so k-fold cross validation would be more suitable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
