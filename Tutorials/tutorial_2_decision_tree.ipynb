{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_entropy(feature):\n",
    "    entropy = 0\n",
    "    categories = feature.unique()\n",
    "    total_count = feature.shape[0]\n",
    "    for category in categories:\n",
    "        cat_count =  feature[feature == category].count()\n",
    "        entropy += cat_count / total_count * math.log(cat_count / total_count, 2) if cat_count > 0 else 0\n",
    "    return -1 * entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_entropy_per_feature(df, feature_name, target_class):\n",
    "    categories = df[feature_name].unique()\n",
    "    entropies = {}\n",
    "    \n",
    "    for i, category in enumerate(categories):\n",
    "        current_entropy = calculate_entropy(df[df[feature_name] == categories[i]][target_class])\n",
    "        entropies[category] = current_entropy\n",
    "        \n",
    "    return entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_feature_entropy(entropies):\n",
    "    for entropy in entropies:\n",
    "        print(\"Entropy({} = {})\\t=\\t{}\".format(feature, entropy, round(entropies[entropy], 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_information_gain(overall_entropy, feature, entropy_per_feature):\n",
    "    local_entropy = 0\n",
    "    for category in entropy_per_feature:\n",
    "        local_entropy += (feature[feature == category].shape[0] / feature.shape[0]) \\\n",
    "                    * entropy_per_feature[category]\n",
    "    return overall_entropy - local_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Name    Hair   Height    Build Lotion     Result\n",
      "1  Sarah  blonde  average    light     no  sunburned\n",
      "2   Dana  blonde     tall  average    yes       none\n",
      "3   Alex   brown    short  average    yes       none\n",
      "4  Annie  blonde    short  average     no  sunburned\n",
      "5  Emily     red  average    heavy     no  sunburned\n",
      "6   Pete   brown     tall    heavy     no       none\n",
      "7   John   brown  average    heavy     no       none\n",
      "8  Katie   brown    short    light    yes       none\n"
     ]
    }
   ],
   "source": [
    "df1 = pandas.read_csv('tuto2_table1.txt', sep=\" \")\n",
    "\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A/ Dataset entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\t What is the entropy of this dataset with respect to the target classlabel Result?\n",
      "Answer\t\t 0.9544\n"
     ]
    }
   ],
   "source": [
    "q1_overall_entropy = calculate_entropy(df1['Result'])\n",
    "\n",
    "print(\"Question:\\t What is the entropy of this dataset with respect to the target classlabel Result?\")\n",
    "print(\"Answer\\t\\t\", round(q1_overall_entropy, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B/ Decision tree & feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Calculate overall dataset entropy (cf Q1/A/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Calculate entropy for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: Hair\n",
      "\n",
      "Entropy(Hair = red)\t=\t-0.0\n",
      "Entropy(Hair = blonde)\t=\t0.9183\n",
      "Entropy(Hair = brown)\t=\t-0.0\n",
      "\n",
      "\n",
      "Feature: Height\n",
      "\n",
      "Entropy(Height = average)\t=\t0.9183\n",
      "Entropy(Height = tall)\t=\t-0.0\n",
      "Entropy(Height = short)\t=\t0.9183\n",
      "\n",
      "\n",
      "Feature: Build\n",
      "\n",
      "Entropy(Build = light)\t=\t1.0\n",
      "Entropy(Build = average)\t=\t0.9183\n",
      "Entropy(Build = heavy)\t=\t0.9183\n",
      "\n",
      "\n",
      "Feature: Lotion\n",
      "\n",
      "Entropy(Lotion = no)\t=\t0.971\n",
      "Entropy(Lotion = yes)\t=\t-0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q1_list_features = list(df1.columns.values)\n",
    "q1_list_features.remove(\"Name\")\n",
    "q1_list_features.remove(\"Result\")\n",
    "\n",
    "q1_target_class = 'Result'\n",
    "\n",
    "q1_entropy_per_feature = {}\n",
    "\n",
    "for feature in q1_list_features:\n",
    "    curr_entropy = calculate_entropy_per_feature(df1, feature, q1_target_class)\n",
    "    q1_entropy_per_feature[feature] = curr_entropy\n",
    "    print(\"Feature:\", feature)\n",
    "    print()\n",
    "    print_feature_entropy(curr_entropy)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculate Information Gain for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature:\t\t Hair\n",
      "Information gain:\t 0.6101\n",
      "\n",
      "Feature:\t\t Height\n",
      "Information gain:\t 0.2657\n",
      "\n",
      "Feature:\t\t Build\n",
      "Information gain:\t 0.0157\n",
      "\n",
      "Feature:\t\t Lotion\n",
      "Information gain:\t 0.3476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q1_information_gain_per_feature = {}\n",
    "\n",
    "for feature in q1_list_features:\n",
    "    curr_information_gain = calculate_information_gain(q1_overall_entropy, \n",
    "                                                       df1[feature],\n",
    "                                                       q1_entropy_per_feature[feature])\n",
    "    q1_information_gain_per_feature[feature] = curr_information_gain\n",
    "    print(\"Feature:\\t\\t\", feature)\n",
    "    print(\"Information gain:\\t\", round(curr_information_gain, 4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Construct the decision tree that would be built with Information Gain for this dataset. Show your work for selection of the root feature in your tree.\n",
      "\n",
      "Answer:\n",
      "\"Hair\" will be selected as it is the feature with the highest IG value. It perfectly classifies the data for Hair=brown & Hair=red. It will be used to split the root node of the tree. The case for Hair=blonde contains (2 sunburned, 1 none). We Can split these into pure child nodes using feature \"Lotion\".\n"
     ]
    }
   ],
   "source": [
    "print(\"Question:\\nConstruct the decision tree that would be built with Information Gain for this dataset. Show your work for selection of the root feature in your tree.\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Answer:\\n\\\"Hair\\\" will be selected as it is the feature with the highest IG value. It perfectly classifies the data for Hair=brown & Hair=red. It will be used to split the root node of the tree. The case for Hair=blonde contains (2 sunburned, 1 none). We Can split these into pure child nodes using feature \\\"Lotion\\\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C/ New element classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dana\n",
      "Hair: Blond\n",
      "Lotion: True\n",
      "Build: average\n",
      "Height: tall\n"
     ]
    }
   ],
   "source": [
    "# hard-coded tree\n",
    "def decision_tree(hair_color, applied_lotion):\n",
    "    if hair_color == \"Blonde\":\n",
    "        if applied_lotion:\n",
    "            return False\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "new_person = {\"Name\": \"Dana\", \n",
    "              \"Hair\": \"Blond\", \n",
    "              \"Height\": \"tall\", \n",
    "              \"Build\":\"average\", \n",
    "              \"Lotion\":True}\n",
    "\n",
    "for data in new_person:\n",
    "    print(\"{}: {}\".format(data, new_person[data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\tUsing your decision tree from (b), how would you classify the following example X?\n",
      "Answer:\t\tnot sunburned\n"
     ]
    }
   ],
   "source": [
    "print(\"Question:\\tUsing your decision tree from (b), how would you classify the following example X?\")\n",
    "sunburned = decision_tree(new_person['Hair'], new_person['Lotion'])\n",
    "if sunburned:   \n",
    "    print(\"Answer:\\t\\t{}\".format(\"sunburned\"))\n",
    "else:\n",
    "    print(\"Answer:\\t\\t{}\".format(\"not sunburned\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Credit  History  Debt  Income    Risk\n",
      "0        1      bad   low   0to30    high\n",
      "1        2      bad  high  30to60    high\n",
      "2        3      bad   low   0to30    high\n",
      "3        4  unknown  high  30to60    high\n",
      "4        5  unknown  high   0to30    high\n",
      "5        6     good  high   0to30    high\n",
      "6        7      bad   low  over60  medium\n",
      "7        8  unknown   low  30to60  medium\n",
      "8        9     good  high  30to60  medium\n",
      "9       10  unknown   low  over60     low\n",
      "10      11  unknown   low  over60     low\n",
      "11      12     good   low  over60     low\n",
      "12      13     good  high  over60     low\n",
      "13      14     good  high  over60     low\n"
     ]
    }
   ],
   "source": [
    "df2 = pandas.read_csv(\"tuto2_table2.txt\", sep=\" \")\n",
    "\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A/ Dataset entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "What is the entropy of this dataset with respect to the target class label Risk based on the 14 examples above? \n",
      "\n",
      "Answer:\n",
      "1.5306\n"
     ]
    }
   ],
   "source": [
    "print(\"Question:\\nWhat is the entropy of this dataset with respect to the target class label Risk based on the 14 examples above? \")\n",
    "\n",
    "q2_overall_entropy = calculate_entropy(df2['Risk'])\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Answer:\\n{}\".format(round(q2_overall_entropy, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B/ Entropy per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy(History = bad)\t=\t0.8113\n",
      "Entropy(History = unknown)\t=\t1.5219\n",
      "Entropy(History = good)\t=\t1.371\n",
      "\n",
      "Entropy(Debt = high)\t=\t1.3788\n",
      "Entropy(Debt = low)\t=\t1.5567\n",
      "\n",
      "Entropy(Income = over60)\t=\t0.65\n",
      "Entropy(Income = 30to60)\t=\t1.0\n",
      "Entropy(Income = 0to30)\t=\t-0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q2_list_features = list(df2.columns.values)\n",
    "q2_list_features.remove(\"Credit\")\n",
    "q2_list_features.remove(\"Risk\")\n",
    "\n",
    "q2_target_class = \"Risk\"\n",
    "\n",
    "q2_entropy_per_feature = {}\n",
    "\n",
    "for feature in q2_list_features:\n",
    "    curr_entropy = calculate_entropy_per_feature(df2, feature, q2_target_class)\n",
    "    q2_entropy_per_feature[feature] = curr_entropy\n",
    "    print_feature_entropy(curr_entropy)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C/ Information gain and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature:\t\t History\n",
      "Information gain:\t 0.2657\n",
      "\n",
      "Feature:\t\t Debt\n",
      "Information gain:\t 0.0629\n",
      "\n",
      "Feature:\t\t Income\n",
      "Information gain:\t 0.9663\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q2_information_gain_per_feature = {}\n",
    "\n",
    "for feature in q2_list_features:\n",
    "    curr_information_gain = calculate_information_gain(q2_overall_entropy, \n",
    "                                                       df2[feature], \n",
    "                                                       q2_entropy_per_feature[feature])\n",
    "    q2_information_gain_per_feature[feature] = curr_information_gain\n",
    "    print(\"Feature:\\t\\t\", feature)\n",
    "    print(\"Information gain:\\t\", round(curr_information_gain, 4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Which one of the descriptive features would be selected by ID3 at the root of a decision tree? Explain your answer. Show all the steps of the calculations.\n",
      "\n",
      "Answer:\n",
      "\"Income\" will be selected as the feature to split as it has the highest IG value.\n"
     ]
    }
   ],
   "source": [
    "print(\"Question:\\nWhich one of the descriptive features would be selected by ID3 at the root of a decision tree? Explain your answer. Show all the steps of the calculations.\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Answer:\\n\\\"Income\\\" will be selected as the feature to split as it has the highest IG value.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D/ Reflections on information gain criterions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"Although information gain is usually a good measure for deciding the relevance of an attribute, it is not perfect. A notable problem occurs when information gain is applied to attributes that can take on a large number of distinct values. For example, suppose that one is building a decision tree for some data describing the customers of a business. Information gain is often used to decide which of the attributes are the most relevant, so they can be tested near the root of the tree. One of the input attributes might be the customer's credit card number. This attribute has a high mutual information, because it uniquely identifies each customer, but we do not want to include it in the decision tree: deciding how to treat a customer based on their credit card number is unlikely to generalise to customers we haven't seen before (overfitting).\"\n",
    "\n",
    "\n",
    "http://en.wikipedia.org/wiki/Information_gain_in_decision_trees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}